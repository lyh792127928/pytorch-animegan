{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a392f84",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python train.py --device 'cpu' --epochs 20 --save_interval 1 --train_photo_path train_photo --dataset shinkai --batch 4 --init_epochs 4 --use_sn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd5019",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python script/video_to_images.py --video-path ./dataset/video/city.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400f6a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python script/edge_smooth.py --dataset shinkai --image-size 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8f1195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight loaded, ready to predict\n",
      "Anime image saved to 1.png\n"
     ]
    }
   ],
   "source": [
    "!python inference_image.py --checkpoint checkpoint/generator_shinkai_24.pth --src 1.jpg --dest 1.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fd976f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight loaded, ready to predict\n",
      "Anime image saved to 1.png\n"
     ]
    }
   ],
   "source": [
    "!python inference_image.py --checkpoint checkpoint/little_anime_face/generator_shinkai_train_24_finetune_10.pth --src 1.jpg --dest 1.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b02bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764971a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c31438b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight loaded, ready to predict\n",
      "Transfroming video dataset/video/test1.mp4, 89 frames, size: [1280, 720]\n",
      "[Errno 32] Broken pipe\n",
      "\n",
      "MoviePy error: FFMPEG encountered the following error while writing file dataset/video/test/1.mp4:\n",
      "\n",
      " b''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "4it [00:33,  8.26s/it]\n",
      "7it [01:09,  9.88s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\MSE\\deep learning\\cartoonize\\pytorchanimegan\\inference_video.py\", line 25, in <module>\n",
      "    main(args)\n",
      "  File \"E:\\MSE\\deep learning\\cartoonize\\pytorchanimegan\\inference_video.py\", line 18, in main\n",
      "    Transformer(args.checkpoint).transform_video(args.src, args.dest,\n",
      "  File \"E:\\MSE\\deep learning\\cartoonize\\pytorchanimegan\\inference.py\", line 137, in transform_video\n",
      "    transform_and_write(frames, frame_count, video_writer)\n",
      "  File \"E:\\MSE\\deep learning\\cartoonize\\pytorchanimegan\\inference.py\", line 105, in transform_and_write\n",
      "    writer.write_frame(img)\n",
      "  File \"D:\\software\\anaconda3\\lib\\site-packages\\moviepy\\video\\io\\ffmpeg_writer.py\", line 136, in write_frame\n",
      "    self.proc.stdin.write(img_array.tobytes())\n",
      "ValueError: write to closed file\n"
     ]
    }
   ],
   "source": [
    "!python inference_video.py --checkpoint checkpoint/generator_shinkai_24.pth --src dataset/video/test1.mp4 --dest dataset/video/test/1.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc969c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ccf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a136e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93a248f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='shinkai')\n",
    "parser.add_argument('--data-dir', type=str, default='dataset')\n",
    "parser.add_argument('--train_photo_path', type=str, default='train_photo')\n",
    "parser.add_argument('--epochs', type=int, default=100)\n",
    "parser.add_argument('--init_epochs', type=int, default=5)\n",
    "parser.add_argument('--batch-size', type=int, default=6)\n",
    "parser.add_argument('--checkpoint_dir', type=str, default='checkpoint')\n",
    "parser.add_argument('--save_image_dir', type=str, default='dataset/predict_photo')\n",
    "parser.add_argument('--gan_loss', type=str, default='lsgan', help='lsgan / hinge / bce')\n",
    "parser.add_argument('--device', type=str, default='cpu')\n",
    "#stor_true为如果命令行有该参数，则该参数设置为True,否则设置为False\n",
    "parser.add_argument('--use_sn', action='store_true')\n",
    "parser.add_argument('--save_interval', type=int, default=5)\n",
    "parser.add_argument('--debug_samples', type=int, default=0)\n",
    "parser.add_argument('--lr_g', type=float, default=2e-4)\n",
    "parser.add_argument('--lr_d', type=float, default=4e-4)\n",
    "parser.add_argument('--init_lr', type=float, default=1e-3)\n",
    "parser.add_argument('--wadvg', type=float, default=10.0, help='Adversarial loss weight for G')\n",
    "parser.add_argument('--wadvd', type=float, default=10.0, help='Adversarial loss weight for D')\n",
    "parser.add_argument('--wcon', type=float, default=1.5, help='Content loss weight')\n",
    "parser.add_argument('--wgra', type=float, default=3.0, help='Gram loss weight')\n",
    "parser.add_argument('--wcol', type=float, default=30.0, help='Color loss weight')\n",
    "parser.add_argument('--d_layers', type=int, default=3, help='Discriminator conv layers')\n",
    "parser.add_argument('--d_noise', action='store_true')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4b52666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.common import load_checkpoint,load_weight\n",
    "from modeling.anime_ganv2 import Generator\n",
    "from modeling.anime_ganv2 import Discriminator\n",
    "import os\n",
    "def load_checkpoint(model, checkpoint_dir, posfix=''):\n",
    "    path = os.path.join(checkpoint_dir, f'{model.name}{posfix}.pth')\n",
    "    return load_weight(model, path)\n",
    "G = Generator('shinkai')\n",
    "D = Discriminator(args).to(args.device)\n",
    "\n",
    "start_e = load_checkpoint(G, 'checkpoint')\n",
    "test = load_checkpoint(D, 'checkpoint')\n",
    "start_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d76ff06e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f782e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53000e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44a905b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92479cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efdf780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05fd2397",
   "metadata": {},
   "source": [
    "# kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4834fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/lyh792127928/pytorch-animegan.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e028c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pytorch-animegan/train.py --device 'cpu' --epochs 20 --save_interval 1 --checkpoint_dir pytorch-animegan/checkpoint --train_photo_path train_photo --data-dir pytorch-animegan/dataset --dataset shinkai --batch 4 --init_epochs 4 --use_sn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e939a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import datetime\n",
    "\n",
    "def file2zip(packagePath, zipPath):\n",
    "    '''\n",
    "  :param packagePath: 文件夹路径\n",
    "  :param zipPath: 压缩包路径\n",
    "  :return:\n",
    "  '''\n",
    "    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n",
    "    for path, dirNames, fileNames in os.walk(packagePath):\n",
    "        fpath = path.replace(packagePath, '')\n",
    "        for name in fileNames:\n",
    "            fullName = os.path.join(path, name)\n",
    "            name = fpath + '\\\\' + name\n",
    "            zip.write(fullName, name)\n",
    "    zip.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 文件夹路径\n",
    "    packagePath = '/kaggle/working/pytorch-animegan/checkpoint'\n",
    "    zipPath = '/kaggle/working/output.zip'\n",
    "    if os.path.exists(zipPath):\n",
    "        os.remove(zipPath)\n",
    "    file2zip(packagePath, zipPath)\n",
    "    print(\"打包完成\")\n",
    "    print(datetime.datetime.utcnow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c36b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2774b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa158e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31864541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbfb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5136305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "def file(p):\n",
    "    p=p\n",
    "    z=os.listdir(p)\n",
    "    for i in z:\n",
    "        \n",
    "        if not (os.path.isfile(os.path.join(p,i))):\n",
    "            pp=os.path.join(p,i)\n",
    "            file(pp)\n",
    "        else:\n",
    "            zz=os.listdir(p)\n",
    "            for k in zz:\n",
    "                paa.append(os.path.join(p,k))\n",
    "if __name__ == '__main__':\n",
    "    p = 'dataset/human_face/origin'\n",
    "    n_p = 'dataset/human_face/style'\n",
    "    paa = []\n",
    "    file(p)\n",
    "    \n",
    "    for f in paa:\n",
    "        try:\n",
    "            shutil.move(f,n_p)\n",
    "        except:\n",
    "            continue\n",
    "    print('移动完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c32c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "p = 'dataset/human_face/origin'\n",
    "n_p = 'dataset/human_face/style'\n",
    "paa = []\n",
    "z=os.listdir(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f7403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python script/resize_image.py --dataset human_face --image_size 224 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d758d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164f597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "145b214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import moviepy.video.io.ffmpeg_writer as ffmpeg_writer\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from modeling.anime_ganv2 import Generator\n",
    "from utils.common import load_weight\n",
    "from utils.image_processing import resize_image, normalize_input, denormalize_input\n",
    "from utils import read_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84587e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfroming video dataset/video/test1.mp4, 89 frames, size: [1280, 720]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "output_dir = 'dataset/video/test'\n",
    "output_path = 'dataset/video/test/1.mp4'\n",
    "input_path = 'dataset/video/test1.mp4'\n",
    "video_clip = VideoFileClip(input_path, audio=False)\n",
    "video_writer = ffmpeg_writer.FFMPEG_VideoWriter(\n",
    "            output_path,\n",
    "            video_clip.size, video_clip.fps, codec=\"libx264\",\n",
    "            preset=\"medium\", bitrate=\"2000k\",\n",
    "            audiofile=input_path, threads=None,\n",
    "            ffmpeg_params=None)\n",
    "total_frames = round(video_clip.fps * video_clip.duration)\n",
    "print(f'Transfroming video {input_path}, {total_frames} frames, size: {video_clip.size}')\n",
    "batch_shape = (batch_size, video_clip.size[1], video_clip.size[0], 3)\n",
    "frame_count = 0\n",
    "frames = np.zeros(batch_shape, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4bebf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_input(images, dtype=None):\n",
    "    images = images * 127.5 + 127.5\n",
    "\n",
    "    if dtype is not None:\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            images = images.type(dtype)\n",
    "        else:\n",
    "            # numpy.ndarray\n",
    "            images = images.astype(dtype)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2eb0b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    images = images.astype(np.float32)\n",
    "\n",
    "    # Normalize to [-1, 1]\n",
    "    images = normalize_input(images)\n",
    "    images = torch.from_numpy(images)\n",
    "\n",
    "    \n",
    "    images = images.cpu()\n",
    "\n",
    "    # Add batch dim\n",
    "    if len(images.shape) == 3:\n",
    "        images = images.unsqueeze(0)\n",
    "\n",
    "    # channel first\n",
    "    images = images.permute(0, 3, 1, 2)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c3725e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image):\n",
    "    with torch.no_grad():\n",
    "        fake = G(preprocess_images(image))\n",
    "        fake = fake.detach().cpu().numpy()\n",
    "        # Channel last\n",
    "        fake = fake.transpose(0, 2, 3, 1)\n",
    "        return fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6821a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_write(frames, count, writer):\n",
    "    anime_images = denormalize_input(transform(frames), dtype=np.uint8)\n",
    "    for i in range(0, count):\n",
    "        img = np.clip(anime_images[i], 0, 255)\n",
    "        writer.write_frame(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aed8ded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 168.61it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-f34a52f08d04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_clip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mframes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mframe_count\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mframe_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#if frame_count == batch_size:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m#transform_and_write(frames, frame_count, video_writer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "for frame in tqdm(video_clip.iter_frames()):\n",
    "    frames[frame_count] = frame\n",
    "    frame_count += 1\n",
    "    #if frame_count == batch_size:\n",
    "        #transform_and_write(frames, frame_count, video_writer)\n",
    "        #frame_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f18a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_images = denormalize_input(transform(frames), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3d2c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.clip(anime_images[0], 0, 255)\n",
    "cv2.imshow('test',img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fbd61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257be77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f10e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
