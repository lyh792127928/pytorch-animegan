{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f7e4d7",
   "metadata": {},
   "source": [
    "# 各个脚本案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2847b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd5019",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python script/video_to_images.py --video-path ./dataset/video/city.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400f6a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python script/edge_smooth.py --dataset shinkai --image-size 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f1195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight loaded, ready to predict\n",
      "Found 4 images in sample/origin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"inference_image.py\", line 30, in <module>\n",
      "    main(args)\n",
      "  File \"inference_image.py\", line 24, in main\n",
      "    transformer.transform_in_dir(args.src, args.dest)\n",
      "  File \"E:\\MSE\\deep learning\\cartoonize\\pytorchanimegan\\inference_onnx.py\", line 80, in transform_in_dir\n",
      "    anime_img = self.transform(image)\n",
      "  File \"E:\\MSE\\deep learning\\cartoonize\\pytorchanimegan\\inference_onnx.py\", line 35, in transform\n",
      "    model_inference = copy.deepcopy(self.model)\n",
      "  File \"D:\\software\\anaconda3\\envs\\openvino\\lib\\copy.py\", line 169, in deepcopy\n",
      "    rv = reductor(4)\n",
      "TypeError: can't pickle CompiledModel objects\n"
     ]
    }
   ],
   "source": [
    "!python inference_image.py --type onnx --checkpoint checkpoint/animegan.onnx --src sample/origin --dest sample/anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8977426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight loaded, ready to predict\n",
      "288\n",
      "160\n",
      "Animation video saved to dataset/video/test/1.mp4\n"
     ]
    }
   ],
   "source": [
    "!python inference_video.py --checkpoint checkpoint/generator_shinkai_24.pth --src dataset/video/city.gif --dest dataset/video/test/1.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "356e9b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx already exist\n"
     ]
    }
   ],
   "source": [
    "!python create_onnx.py --checkpoint checkpoint/freeze_d_face/generator_shinkai_train_24_finetune_10.pth --output_onnx checkpoint/animegan_freeze_d_face_finetune.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b0bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f58232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc435ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b458b93",
   "metadata": {},
   "source": [
    "# 查看checkpoint训练批数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "93a248f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='shinkai')\n",
    "parser.add_argument('--device', type=str, default='cpu')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4b52666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.common import load_checkpoint,load_weight\n",
    "from modeling.anime_ganv2 import Generator\n",
    "from modeling.anime_ganv2 import Discriminator\n",
    "import os\n",
    "def load_checkpoint(model, checkpoint_dir, posfix=''):\n",
    "    path = os.path.join(checkpoint_dir, f'{model.name}{posfix}.pth')\n",
    "    return load_weight(model, path)\n",
    "G = Generator('shinkai')\n",
    "D = Discriminator(args).to(args.device)\n",
    "\n",
    "start_e = load_checkpoint(G, 'checkpoint')\n",
    "test = load_checkpoint(D, 'checkpoint')\n",
    "start_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f782e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05fd2397",
   "metadata": {},
   "source": [
    "# kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4834fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/lyh792127928/pytorch-animegan.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e028c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pytorch-animegan/train.py --device 'cpu' --epochs 20 --save_interval 1 --checkpoint_dir pytorch-animegan/checkpoint --train_photo_path train_photo --data-dir pytorch-animegan/dataset --dataset shinkai --batch 4 --init_epochs 4 --use_sn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e939a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import datetime\n",
    "\n",
    "def file2zip(packagePath, zipPath):\n",
    "    '''\n",
    "  :param packagePath: 文件夹路径\n",
    "  :param zipPath: 压缩包路径\n",
    "  :return:\n",
    "  '''\n",
    "    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n",
    "    for path, dirNames, fileNames in os.walk(packagePath):\n",
    "        fpath = path.replace(packagePath, '')\n",
    "        for name in fileNames:\n",
    "            fullName = os.path.join(path, name)\n",
    "            name = fpath + '\\\\' + name\n",
    "            zip.write(fullName, name)\n",
    "    zip.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 文件夹路径\n",
    "    packagePath = '/kaggle/working/pytorch-animegan/checkpoint'\n",
    "    zipPath = '/kaggle/working/output.zip'\n",
    "    if os.path.exists(zipPath):\n",
    "        os.remove(zipPath)\n",
    "    file2zip(packagePath, zipPath)\n",
    "    print(\"打包完成\")\n",
    "    print(datetime.datetime.utcnow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c36b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28923a19",
   "metadata": {},
   "source": [
    "# 移动文件，用于图片批量移动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5136305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "def file(p):\n",
    "    p=p\n",
    "    z=os.listdir(p)\n",
    "    for i in z:\n",
    "        \n",
    "        if not (os.path.isfile(os.path.join(p,i))):\n",
    "            pp=os.path.join(p,i)\n",
    "            file(pp)\n",
    "        else:\n",
    "            zz=os.listdir(p)\n",
    "            for k in zz:\n",
    "                paa.append(os.path.join(p,k))\n",
    "if __name__ == '__main__':\n",
    "    p = 'dataset/human_face/origin'\n",
    "    n_p = 'dataset/human_face/style'\n",
    "    paa = []\n",
    "    file(p)\n",
    "    \n",
    "    for f in paa:\n",
    "        try:\n",
    "            shutil.move(f,n_p)\n",
    "        except:\n",
    "            continue\n",
    "    print('移动完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c32c303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56ee871b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 256, 256]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,3,256,256)\n",
    "#x.size()[1:]\n",
    "torch.tensor(x.size()[1:]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df959d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd85844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70c79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201c24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5e81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9045f74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.common import load_weight\n",
    "from modeling.anime_ganv2 import Generator\n",
    "import torch\n",
    "torch_model = Generator()\n",
    "load_weight(torch_model,'checkpoint/generator_shinkai.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed16a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_onnx = 'checkpoint/animegan.onnx'\n",
    "print(\"==> Exporting model to ONNX format at '{}'\".format(output_onnx))\n",
    "input_names = [\"input0\"]\n",
    "output_names = [\"output0\"]\n",
    "# 转ONNX 动态输入转换代码\n",
    "dynamic_axes= {'input0':[0, 2, 3], 'output0':[0,2,3]} #数字0，1等是指张量的维度，表示哪个维度需要动态输入\n",
    "inputs = torch.randn(1, 3, 256, 256)\n",
    "onnx_model = torch.onnx.export(torch_model, inputs, output_onnx,input_names=input_names, output_names=output_names,opset_version=11, dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1e298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_input(images, dtype=None):\n",
    "    '''\n",
    "    [-1, 1] -> [0, 255]\n",
    "    '''\n",
    "    images = images * 127.5 + 127.5\n",
    "\n",
    "    if dtype is not None:\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            images = images.type(dtype)\n",
    "        else:\n",
    "            # numpy.ndarray\n",
    "            images = images.astype(dtype)\n",
    "\n",
    "    return images\n",
    "def normalize_input(images):\n",
    "    '''\n",
    "    [0, 255] -> [-1, 1]\n",
    "    '''\n",
    "    return images / 127.5 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7967516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    images = images.astype(np.float32)\n",
    "\n",
    "    # Normalize to [-1, 1]\n",
    "    images = normalize_input(images)\n",
    "    #images = torch.from_numpy(images)\n",
    "    # Add batch dim\n",
    "    if len(images.shape) == 3:\n",
    "        images = np.expand_dims(images, 0)\n",
    "    # channel first\n",
    "    images = images.transpose(0, 3, 1, 2)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be276fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_tensor = preprocess_images(image)\\nresults = model.infer_new_request({0: input_tensor})\\npredictions = next(iter(results.values()))\\npred = denormalize_input(predictions.transpose(0, 2, 3, 1)[0],dtype=np.uint8)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#openvino\n",
    "from openvino.runtime import Core\n",
    "import openvino.runtime as ov\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    " \n",
    "core = Core()\n",
    "model=r\"checkpoint\\animegan.onnx\"\n",
    "model = core.compile_model(model=model)\n",
    "image = cv2.imread('sample/origin/face.jpg')[:,:,::-1]\n",
    "'''\n",
    "input_tensor = preprocess_images(image)\n",
    "results = model.infer_new_request({0: input_tensor})\n",
    "predictions = next(iter(results.values()))\n",
    "pred = denormalize_input(predictions.transpose(0, 2, 3, 1)[0],dtype=np.uint8)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400a6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_request = model.create_infer_request()\n",
    "input_tensor = preprocess_images(image)\n",
    "input_tensor = ov.Tensor(input_tensor)\n",
    "infer_request.set_input_tensor(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_request.start_async()\n",
    "infer_request.wait()\n",
    "# Get output tensor for model with one output\n",
    "output = infer_request.get_output_tensor()\n",
    "output_buffer = output.data\n",
    "# output_buffer[] - accessing output tensor data\n",
    "pred = denormalize_input(output_buffer.transpose(0, 2, 3, 1)[0],dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907617cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Tensor: shape{0, 0, 0, 0} type: f32>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_request.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c32fdc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('1.jpg',pred[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_request.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7cbd2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor: shape{1, 3, 411, 430} type: f32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbed282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f43f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f0caef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 images in sample/origin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "img_dir = 'sample/origin'\n",
    "dest_dir = 'sample/anime'\n",
    "files = os.listdir(img_dir)\n",
    "print(f'Found {len(files)} images in {img_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in tqdm(files):\n",
    "    image = cv2.imread(os.path.join(img_dir, fname))[:,:,::-1]\n",
    "    #anime_img = transform(image)\n",
    "    input_tensor = preprocess_images(image)\n",
    "    results = model.infer_new_request({0: input_tensor,1:input_tensor})\n",
    "    predictions = next(iter(results.values()))\n",
    "    pred = denormalize_input(predictions.transpose(0, 2, 3, 1)[0],dtype=np.uint8)\n",
    "    anime_img = pred[:,:,::-1]\n",
    "    ext = fname.split('.')[-1]\n",
    "    fname = fname.replace(f'.{ext}', '')\n",
    "    cv2.imwrite(os.path.join(dest_dir, f'{fname}_anime.jpg'), anime_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a36894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ac09a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f65ea095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c86631a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6999c0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
